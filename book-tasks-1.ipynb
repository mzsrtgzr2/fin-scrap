{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utils.spark import write_stream_to_console, to_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Caskroom/miniconda/base/envs/personal/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/26 15:22:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName('book1')\\\n",
    "    .config('spark.sql.shuffle.partitions', 1)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "retail_schema = StructType([\n",
    "    StructField('InvoiceNo', LongType()),\n",
    "    StructField('StockCode', StringType()),\n",
    "    StructField('Description', StringType()),\n",
    "    StructField('Quantity', IntegerType()),\n",
    "    StructField('InvoiceDate', TimestampType()),\n",
    "    StructField('UnitPrice', FloatType()),\n",
    "    StructField('CustomerID', FloatType()),\n",
    "    StructField('Country', StringType()),\n",
    "])\n",
    "\n",
    "retail_stream = spark.readStream\\\n",
    "    .schema(retail_schema)\\\n",
    "    .format('csv')\\\n",
    "    .option('header', True)\\\n",
    "    .load('/Users/mrot/workspace/trade-bigdata/Spark-The-Definitive-Guide/data/retail-data/by-day/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/26 15:22:37 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/z7/5594v_fd6q337t3912lvx6rw0000gn/T/temporary-8a3bcf97-d30a-4fb8-af6a-e1e73e92e5b7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fd41dd81110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_transactions_stream = retail_stream.select(\n",
    "    col('CustomerID'),\n",
    "    (col('UnitPrice')*col('Quantity')).alias('total_price'),\n",
    "    col('InvoiceDate')\n",
    ")\n",
    "\n",
    "customer_transactions_1h_stream = customer_transactions_stream\\\n",
    "    .groupBy(\n",
    "        col('CustomerID'), window(col('InvoiceDate'), '1 hour')\n",
    "    )\\\n",
    "    .sum('total_price')\\\n",
    "    .withColumnRenamed('sum(total_price)', 'total_price')\n",
    "\n",
    "to_table('customers_transactions', customer_transactions_1h_stream)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|       total_price|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 11:00...| 31698.15982055664|\n",
      "|   18102.0|{2011-09-15 15:00...|31661.539978027344|\n",
      "|   17450.0|{2011-09-20 10:00...|29712.739707946777|\n",
      "|   18102.0|{2010-12-07 16:00...|25920.370010375977|\n",
      "|   14646.0|{2011-10-20 12:00...| 25833.56008052826|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/26 15:57:17 WARN FileStreamSource: Listed 305 file(s) in 2298 ms\n",
      "21/10/26 15:58:11 WARN FileStreamSource: Listed 305 file(s) in 3727 ms\n",
      "21/10/26 16:19:22 WARN FileStreamSource: Listed 305 file(s) in 2532 ms\n",
      "21/10/26 16:23:05 WARN FileStreamSource: Listed 305 file(s) in 2310 ms\n",
      "21/10/26 16:23:14 WARN FileStreamSource: Listed 305 file(s) in 2640 ms\n",
      "21/10/26 16:23:22 WARN FileStreamSource: Listed 305 file(s) in 7667 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "    select * from customers_transactions\n",
    "    where CustomerID is not null\n",
    "    order by total_price desc\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a27c9f2c5f0318bf29c3c04b8e213f35e3a5c52524760be1d78ad640459f30c1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('personal': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
